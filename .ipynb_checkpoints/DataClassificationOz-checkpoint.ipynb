{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6812d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a8d8e",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180ee503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615341, 14)\n"
     ]
    }
   ],
   "source": [
    "proportionNeutral = 1.1\n",
    "\n",
    "oz = pd.read_csv('OzRaw.csv', parse_dates = [0])\n",
    "oz['date'] = oz.date.dt.date\n",
    "print(oz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5347018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083fd49",
   "metadata": {},
   "source": [
    "# Define important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c632b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetScores(df, verbose = True):\n",
    "    '''\n",
    "    df is pandas dataframe with column named 'content' with the tweets\n",
    "    proprotion neutral (double) \n",
    "    \n",
    "    Returns:\n",
    "        df with positive, negative, neutral, compound and polarity\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    \n",
    "    #Get the pos, neg and neu\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['rating'] = df['content'].apply(analyzer.polarity_scores)\n",
    "    df = pd.concat([df.drop(['rating'], axis=1), df['rating'].apply(pd.Series)], axis=1)\n",
    "    \n",
    "    #Get polarity and subjectivity\n",
    "    df['pol'] = df['content'].apply(lambda tweet: TextBlob(tweet).sentiment)\n",
    "    dfAux = df['pol'].apply(pd.Series)\n",
    "    dfAux.columns = ['polarity', 'subjectivity']\n",
    "    df = pd.concat([df.drop(['pol'], axis=1), dfAux], axis=1)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def tweetScoresInneficient(df, verbose = True):\n",
    "    '''\n",
    "    df is pandas dataframe with column named 'content' with the tweets\n",
    "    proprotion neutral (double) \n",
    "    \n",
    "    Returns:\n",
    "        df with positive, negative, neutral, compound and polarity\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['neg'] = 0\n",
    "    df['pos'] = 0\n",
    "    df['neu'] = 0\n",
    "    df['compound'] = 0\n",
    "    df['polarity'] = 0\n",
    "    \n",
    "    n = df.shape[0]\n",
    "\n",
    "    if verbose: print('Number of rows: {}'.format(n))\n",
    "\n",
    "    for i in range(n):\n",
    "        analysis = TextBlob(df.content[i])\n",
    "        polarity = analysis.sentiment.polarity\n",
    "\n",
    "        score = SentimentIntensityAnalyzer().polarity_scores(df.content[i])\n",
    "\n",
    "        df.loc[df.index[i], 'neg'] = score['neg']\n",
    "        df.loc[df.index[i], 'pos'] = score['pos']\n",
    "        df.loc[df.index[i], 'neu'] = score['neu']\n",
    "        df.loc[df.index[i], 'compound'] = score['compound']\n",
    "        df.loc[df.index[i], 'polarity'] = polarity\n",
    "        \n",
    "        if ((i % n == 0) & (i != 0)) & verbose:\n",
    "            print('Observation {}'.format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "def classifyTweets(df, proportionNeutral = proportionNeutral):\n",
    "    '''\n",
    "    Given a dataframe that already has columns neu, pos and neg, this classifies whether the tweet is positive, negative\n",
    "    or neutral.\n",
    "    \n",
    "    df (pd.DataFrame)\n",
    "    proportion neutral (double): if neu >= proportion neutral, it is classified directly as neutral. \n",
    "    '''\n",
    "    \n",
    "    df['positive'] = 0\n",
    "    df['negative'] = 0\n",
    "    df['neutral'] = 0\n",
    "\n",
    "    df.loc[(df['neu'] < proportionNeutral) & (df['pos'] > df['neg']), 'positive'] = 1\n",
    "\n",
    "    df.loc[(df['neu'] < proportionNeutral) & (df['pos'] < df['neg']), 'negative'] = 1\n",
    "\n",
    "    df.loc[(df['neu'] < proportionNeutral) & (df['pos'] == df['neg']), 'neutral'] = 1\n",
    "    df.loc[(df['neu'] >= proportionNeutral), 'neutral'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def createClassifier(model_name = model_name, cuda = 0):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True, num_labels=3) \n",
    "    #from_pt=True, because this model only exists in PyTorch\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Combining tokenizer and model into one classifier\n",
    "    classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device = cuda)\n",
    "    return classifier\n",
    "\n",
    "def classifyTweetsBert(df, classifier):\n",
    "    # .apply and loop took the same amount of time\n",
    "    df = df.copy()\n",
    "    \n",
    "    label = []\n",
    "    prob = []\n",
    "    for idx, text in enumerate(df['content']):\n",
    "        result = classifier(text)[0]\n",
    "        label.append( result['label'] )\n",
    "        prob.append( result['score'] )\n",
    "        \n",
    "        if idx % 10_000 == 0 :\n",
    "            print(idx)\n",
    "\n",
    "    df['BERT_sentiment'] = label\n",
    "    df['BERT_prob'] = prob\n",
    "    \n",
    "    return df\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2731ba",
   "metadata": {},
   "source": [
    "# Define the classifier (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35f109",
   "metadata": {},
   "source": [
    "# Start the analysis with TextBlob and nltk sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4d0d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "oz = tweetScores(oz)\n",
    "oz = classifyTweets(oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "oz.to_csv('OzClassified1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54ffca9",
   "metadata": {},
   "source": [
    "# Analyze with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f9bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "#Create the classifier\n",
    "classifier = createClassifier(cuda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec30f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (137 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "Wall time: 15h 33min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oz = classifyTweetsBert(oz, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0993e37",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0d3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "oz.to_csv('OzClassified2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
